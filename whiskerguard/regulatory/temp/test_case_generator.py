from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import time

MODEL_NAME = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªèµ„æ·±è½¯ä»¶æµ‹è¯•ä¸“å®¶ã€‚
æ ¹æ®ä¸‹é¢çš„åŠŸèƒ½éœ€æ±‚ï¼Œå¸®æˆ‘æ‹†è§£å‡ºç®€çŸ­çš„æµ‹è¯•ç”¨ä¾‹å’Œå¯¹åº”çš„ Selenium+Pytest è„šæœ¬æ¨¡æ¿ã€‚
éœ€æ±‚æè¿°ï¼š
\"\"\"
{requirement}
\"\"\"

è¯·è¾“å‡ºï¼š
1. æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ï¼ˆæµ‹è¯•ç‚¹ä¸é¢„æœŸç»“æœï¼‰
2. å¯¹åº”çš„ Python è„šæœ¬æ¨¡æ¿
"""

def generate_test_artifacts(requirement_text: str) -> str:
    prompt = PROMPT_TEMPLATE.format(requirement=requirement_text)
    print("ğŸ”„ å¼€å§‹ç”Ÿæˆâ€¦â€¦")
    start = time.time()
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256)
    outputs = model.generate(**inputs, max_new_tokens=200)
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œç”¨æ—¶ {time.time() - start:.1f}s")
    return result

# if __name__ == "__main__":
#     sample_req = "ç™»å½•é¡µé¢ï¼šæ­£ç¡®ç”¨æˆ·åå¯†ç è·³è½¬ï¼Œé”™è¯¯æç¤ºå‡ºç°"
#     print(generate_test_artifacts(sample_req))
if __name__ == "__main__":
    sample_req = "ç™»å½•é¡µé¢ï¼šæ­£ç¡®ç”¨æˆ·åå¯†ç è·³è½¬ï¼Œé”™è¯¯æç¤ºå‡ºç°"
    result = generate_test_artifacts(sample_req)
    # æ‰“å°å®Œæ•´ reprï¼ŒæŸ¥çœ‹å®ƒæ˜¯ä¸æ˜¯ç©ºå­—ç¬¦ä¸²
    print("ğŸ” raw result repr:", repr(result))
    print("ğŸ”¢ result é•¿åº¦:", len(result))
    print("ğŸ“„ result å†…å®¹:\n", result)
